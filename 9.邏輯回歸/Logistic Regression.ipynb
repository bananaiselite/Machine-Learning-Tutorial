{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistice Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "記得我們之前所講的線性回歸(linear regression)，線性回歸模型幫助我們很好的找出自變數(independent variable)和因變數(dependent variable)，而我們要找出的target是一個連續性的輸出，例如：房價，分數，股價等等。\n",
    "\n",
    "在現實生活中除了預測連續性的數值以外，還充斥著很多分類(classification)的問題，可以用來做分類的模型，被我們稱為分類模型(Classification model)，例如此前講過的Bayes Classifier就是一種。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這次要介紹的是由Linear Regress所演變出來的分類模型，邏輯回歸(Logistic Regression)模型\n",
    "邏輯迴歸是用來處理分類問題，目標是找到一條直線可以將資料做分類\n",
    "\n",
    "<img src=\"pics/Logistic regression v.s. Linear Regression.png\" alt=\"Logistic regression v.s. Linear Regression\" style=\"width: 600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic Regression 雖然被稱為迴歸，但實際上是分類模型，並常用於二分類。Logistic Regression 因其簡單、可並行化、可解釋強深受工業界喜愛。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們先來複習一下回歸，他寫作一個人人都學過的方程式：\n",
    "\n",
    "𝑦 = 𝑤<sub>0</sub>+𝑤<sub>1</sub> 𝑥<sub>1</sub>+𝑤<sub>2</sub> 𝑥<sub>2</sub>+ …𝑤<sub>n</sub> 𝑥<sub>n</sub>\n",
    "\n",
    "在機器學習中我們用𝜃表示模型的參數，所以式子也可以寫成\n",
    "\n",
    "𝑦 = 𝜃<sub>0</sub>+𝜃<sub>1</sub> 𝑥<sub>1</sub>+𝜃<sub>2</sub> 𝑥<sub>2</sub>+ …𝜃<sub>n</sub> 𝑥<sub>n</sub>\n",
    "\n",
    "𝜃<sub>0</sub>就是截距(intercept)，𝜃<sub>1</sub>~𝜃<sub>n</sub>被稱為係數(coefficient)，如果我們用矩陣來表達方程式，則可以表示為\n",
    "\n",
    "<img src=\"pics/array expression.png\" alt=\"array expression\" style=\"width: 300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "線性回歸的任務，就是利用上面的式子來映射出x和y之間的關係，而預測的核心就是找出模型參數: 𝜃<sup>T</sup>和𝜃<sub>0</sub>，最常使用的方法就是最小平方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "線性回歸可以通過輸入特徵矩陣X輸出**連續型**的標籤值。\n",
    "\n",
    "那如果我們今天的標籤值y為**離散型**，尤其是滿足0-1分佈的離散型，那要怎麼辦，這是後我們會引進聯繫函數(Contact function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*聯繫函數(Contact function)*\n",
    "\n",
    "在回歸模型中，當標籤值y其自然參數的期望無法直接與預測值建立相等的關係，因此引進連接函數，作為建立這二者相等分佈的橋樑。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "邏輯回歸中的聯繫函數會將ｙ透過g(y)的轉換，令值分佈(0,1)之間，也就是說當g(y)靠近0，則該類別就是0，g(y)靠近1，則該類別就是1，這樣我們就得到了一個二元分類模型，在邏輯回歸中，聯繫函數為sigmoid。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid函數是Logistic函數的一種特殊形式，通常以σ (x)或sig (x)來表示\n",
    "\n",
    "<img src=\"pics/sigmoid.png\" alt=\"sigmoid\" style=\"width: 500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid是一個S型的函數，當X越趨近無窮，結果也會用趨近於1，反之亦然，所以他能夠將任何實數都映射到(0,1)區間，所以適合用於二元分類的函數。\n",
    "\n",
    "也因為這個性質，Sigmoid有時候也會被當成一種normalized的方式，類似前面教的MinMaxScaler，是屬於\"縮放\"的功能，最大的差別是Minmax是會真的出現0和1的值，而sigmoid只能無限趨近於0和1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def sigmoid(x):\n",
    "    return 1/(1+math.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "當我們今天把sigomid帶入標籤值y則會寫成\n",
    "\n",
    "<img src=\"pics/sigmoid2.png\" alt=\"sigmoid2\" style=\"width: 150px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們先來了解發生比(Odds)，是指該事件的發生與不發生的機率，又稱為勝算\n",
    "公式為 p/(1-p)，p是該事件發生的機率。\n",
    "\n",
    "由於𝜎(x)和1-𝜎(x)相加必為1，因此可以去計算odds，並取對數。\n",
    "\n",
    "<img src=\"pics/sigmoid odds.png\" alt=\"sigmoid odds\" style=\"width: 400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不難發現，sigmoid的對數結果其實就是我們的線性回歸，所以取其對數odds的結果來做擬合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**邏輯回歸返回的是什麼？**\n",
    "\n",
    "我們讓使用sigmoid讓回歸的結果能逼近0或1，並且此時𝜎(x)和1-𝜎(x)相加為1，我們可以看作二項式的機率，𝜎(x)為被預測為1的機率，1-𝜎(x)則為被預測為0的機率，這樣的說法是否合理?\n",
    "\n",
    "機率是衡量事件發生的可能性數值，儘管Logistic regression的值取在(0,1)之間，相加之值也等於1，但是光憑這樣也不足以支撐返回值為機率的說法，就像Minmaxscaler也能將數值壓縮至[0,1]之間，並且任意特徵的取值，1-x和x相加也為1，但我們卻不認為該值為機率。\n",
    "\n",
    "因此邏輯回歸返回機率這說法是不夠嚴謹的，但是長年以來大家都是以返回機率的方式去理解Logistic regression，並且這樣在使用它，因此可以說，Logistic regression返回的本質上不是機率，卻可以當成機率使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import pandas\n",
    "import pandas as pd\n",
    "# load dataset\n",
    "df = pd.read_csv(\"diabetes.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"Outcome\", axis=1) # Features\n",
    "y = df[\"Outcome\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import scipy.special  # For the sigmoid function\n",
    "\n",
    "class LogisticReg:\n",
    "    # Initialize training parameters\n",
    "    def __init__(self, learning_rate=0.001, n_iters=10000):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        # Sigmoid activation function\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        # Initialize weights with zeros\n",
    "        self.weights = np.zeros(n_features)\n",
    "        # Initialize bias\n",
    "        self.bias = 0\n",
    "\n",
    "        # Collect weight and bias changes during training\n",
    "        self.weights_changes = [self.weights]\n",
    "        self.bias_changes = [self.bias]\n",
    "\n",
    "        # Gradient descent\n",
    "        for _ in range(self.n_iters):\n",
    "            # Compute the linear combination of inputs and weights\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            # Apply sigmoid activation function to get probabilities\n",
    "            y_predicted = self.sigmoid(linear_model)\n",
    "\n",
    "            # Calculate gradients for weights and bias\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "\n",
    "            # Update weights and bias\n",
    "            self.weights -= self.lr * dw\n",
    "            self.weights_changes.append(copy.deepcopy(self.weights))\n",
    "            self.bias -= self.lr * db\n",
    "            self.bias_changes.append(copy.deepcopy(self.bias))\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predict probabilities for the positive class\n",
    "        y_probabilities = self.sigmoid(np.dot(X, self.weights) + self.bias)\n",
    "        # Threshold probabilities to get binary predictions (0 or 1)\n",
    "        y_predicted = np.where(y_probabilities > 0.5, 1, 0)\n",
    "        return y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticReg()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.640625"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the class\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# instantiate the model (using the default parameters)\n",
    "logreg = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# fit the model with data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8229166666666666"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "手刻的Logistic Regression得出的結果與sklearn有些微的出入，原因是因為我們使用的Gradient descent。\n",
    "\n",
    "而sklearn使用的方法為\n",
    "**solver{‘lbfgs’, ‘liblinear’, ‘newton-cg’, ‘newton-cholesky’, ‘sag’, ‘saga’}, default=’lbfgs’**，並且有加入regulation l2。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後簡單的總結一下邏輯回歸的使用場景和優缺點：\n",
    "\n",
    "邏輯迴歸（Logistic Regression）主要解決二分類問題，可以看做表示某件事情發生的機率\n",
    "\n",
    "比如：\n",
    "\n",
    "* 一封郵件是垃圾郵件的可能性（是、不是）\n",
    "* 你買一件商品的可能性（買、不買）\n",
    "* 廣告被點擊的可能性（點、不點）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "優點：\n",
    "\n",
    "* 實現簡單，廣泛的應用於工業問題；\n",
    "* 分類時計算量非常小，速度很快，儲存資源低；\n",
    "* 便利的觀測樣本機率分數；\n",
    "* 多重共線性並不是問題，它可以結合L2正規化來解決這個問題；\n",
    "* 計算成本低，易於理解；\n",
    "\n",
    "缺點：\n",
    "\n",
    "* 能很好地處理大量多類特徵或變數時，邏輯迴歸的表現不是很好；\n",
    "* 容易欠擬合，一般準確度不太高\n",
    "* 只能處理二元分類問題（在此基礎上衍生出來的softmax可以用於多分類, 且必須線性可分；\n",
    "* 只能處理線性關係，對於非線性特徵，需要進行轉換；"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
